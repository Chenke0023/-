#!/usr/bin/env python3
"""ä» ai-rss-filter æ•°æ®åº“è¯»å–å¹¶æ‰¹é‡è¿‡æ»¤æ–°é—»"""

import sqlite3
import json
import time
import os
from datetime import datetime
from openai import OpenAI

DB_PATH = os.getenv("DB_PATH", "./ai-rss-filter/data/rss_data.db")
API_URL = os.getenv("OPENAI_BASE_URL", "https://ai.hybgzs.com/v1")
API_KEY = os.getenv("OPENAI_API_KEY")
MODEL = os.getenv("MODEL_NAME", "gemini-3-flash-preview")

if not API_KEY:
    raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

BATCH_SIZE = 50
BATCH_DELAY = 10
REQUEST_DELAY = 0.2
MAX_RETRIES = 3
RETRY_DELAY = 5

def is_relevant_with_llm(title, summary, client):
    prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹æ–°é—»æ˜¯å¦ä¸è¿™äº›ä¸»é¢˜ç›¸å…³ï¼šSocial networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰ã€‚

æ–°é—»æ ‡é¢˜: {title}
æ–°é—»æ‘˜è¦: {summary if summary else 'æ— æ‘˜è¦'}

è¯·åªå›ç­” YES æˆ– NOï¼Œä¸éœ€è¦è§£é‡Šã€‚"""

    for retry in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è¿‡æ»¤å™¨ï¼Œåˆ¤æ–­æ–°é—»æ˜¯å¦ä¸ç‰¹å®šä¸»é¢˜ç›¸å…³ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=10
            )

            answer = response.choices[0].message.content.strip().upper()
            return answer == "YES"
        except Exception as e:
            if '429' in str(e) and retry < MAX_RETRIES - 1:
                print(f"  â³ API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {RETRY_DELAY} ç§’åé‡è¯•... ({retry + 1}/{MAX_RETRIES})")
                time.sleep(RETRY_DELAY)
                continue
            else:
                print(f"  âŒ LLM åˆ¤æ–­å¤±è´¥: {e}")
                return False
    return False

def main():
    print("ğŸš€ æ‰¹é‡å¤„ç†æ–°é—»ï¼ˆæ¯æ‰¹ 50 æ¡ï¼Œé¿å… API é€Ÿç‡é™åˆ¶ï¼‰\n")

    client = OpenAI(api_key=API_KEY, base_url=API_URL)

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute("SELECT title, link, published, summary FROM entries ORDER BY published DESC")
    all_entries = cursor.fetchall()

    print(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰ {len(all_entries)} æ¡æ–°é—»")
    print(f"ğŸ”„ åˆ†æ‰¹å¤„ç†: æ¯æ‰¹ {BATCH_SIZE} æ¡ï¼Œæ‰¹æ¬¡é—´éš” {BATCH_DELAY} ç§’\n")

    total_batches = (len(all_entries) + BATCH_SIZE - 1) // BATCH_SIZE
    all_relevant = []

    for batch_num in range(total_batches):
        start_idx = batch_num * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(all_entries))
        batch_entries = all_entries[start_idx:end_idx]

        print(f"{'='*60}")
        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches}")
        print(f"{'='*60}")
        print(f"èŒƒå›´: {start_idx + 1} - {end_idx} (å…± {len(batch_entries)} æ¡)\n")

        batch_relevant = []

        for i, entry in enumerate(batch_entries, start_idx + 1):
            title, link, published, summary = entry

            print(f"[{i}/{len(all_entries)}] {title[:60]}...", end=" ")

            summary = summary[:200] if summary else ''

            if is_relevant_with_llm(title, summary, client):
                batch_relevant.append({
                    'title': title,
                    'link': link,
                    'published': published,
                    'summary': summary
                })
                print("âœ… ç›¸å…³")
            else:
                print("â­ï¸  ä¸ç›¸å…³")

            time.sleep(REQUEST_DELAY)

        all_relevant.extend(batch_relevant)

        print(f"\nâœ… æœ¬æ‰¹æ¬¡å®Œæˆ: æ‰¾åˆ° {len(batch_relevant)} æ¡ç›¸å…³æ–°é—»")
        print(f"ğŸ“Š ç´¯è®¡ç›¸å…³: {len(all_relevant)}/{len(all_entries)} æ¡ ({len(all_relevant)/len(all_entries)*100:.1f}%)\n")

        if batch_num < total_batches - 1:
            print(f"â³ ç­‰å¾… {BATCH_DELAY} ç§’åç»§ç»­ä¸‹ä¸€æ‰¹...\n")
            time.sleep(BATCH_DELAY)

    print(f"{'='*60}")
    print(f"âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"{'='*60}\n")

    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - æ•°æ®åº“æ€»æ•°: {len(all_entries)} æ¡")
    print(f"   - ç›¸å…³æ–°é—»: {len(all_relevant)} æ¡")
    print(f"   - ç›¸å…³æ¯”ä¾‹: {len(all_relevant)/len(all_entries)*100:.2f}%")

    all_relevant.sort(key=lambda x: x['published'], reverse=True)

    md_content = f'''# ç›¸å…³æ–°é—»è¿‡æ»¤ç»“æœ

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
**è¿‡æ»¤ä¸»é¢˜**: Social networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€Live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€Mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰
**ç›¸å…³æ–°é—»æ•°é‡**: {len(all_relevant)} æ¡
**æ•°æ®åº“æ€»æ–°é—»æ•°**: {len(all_entries)} æ¡
**è¿‡æ»¤æ¯”ä¾‹**: {len(all_relevant)/len(all_entries)*100:.2f}%

---

## ğŸ“° æ–°é—»åˆ—è¡¨

'''

    for i, news in enumerate(all_relevant, 1):
        md_content += f'''
### {i}. {news['title']}

**é“¾æ¥**: [{news['link']}]({news['link']})
**å‘å¸ƒæ—¶é—´**: {news['published'][:16]}
**æ‘˜è¦**: {news.get('summary', 'æ— ')}

---

'''

    md_content += f'''

---

## ğŸ“Š å¤„ç†ç»Ÿè®¡

| ç»Ÿè®¡é¡¹ | æ•°å€¼ |
|---------|------|
| æ•°æ®åº“æ€»æ–°é—»æ•° | {len(all_entries)} æ¡ |
| æœ¬æ‰¹å¤„ç†æ•°é‡ | {len(all_entries)} æ¡ |
| æ‰¾åˆ°ç›¸å…³æ–°é—» | {len(all_relevant)} æ¡ |
| ç›¸å…³æ¯”ä¾‹ | {len(all_relevant)/len(all_entries)*100:.2f}% |
| LLM æ¨¡å‹ | {MODEL} |
| API æä¾›å•† | ai.hybgzs.com |

---

*æœ¬æŠ¥å‘Šç”± AI RSS è¿‡æ»¤å™¨è‡ªåŠ¨ç”Ÿæˆ*
*æ•°æ®æ¥æº: 24 ä¸ª RSS æº*
*å¤„ç†æ‰¹æ¬¡: {total_batches} æ‰¹*
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
'''

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_file = f"filtered_news_{timestamp}.json"
    md_file = f"ç›¸å…³æ–°é—»_{timestamp}.md"

    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_relevant, f, ensure_ascii=False, indent=2)

    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"\nâœ… ç»“æœå·²ä¿å­˜:")
    print(f"   ğŸ“„ JSON: {json_file}")
    print(f"   ğŸ“„ Markdown: {md_file}")

    conn.close()

if __name__ == "__main__":
    main()
