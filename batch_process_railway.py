#!/usr/bin/env python3
"""RSS æ–°é—»è¿‡æ»¤å™¨ - Railway ç‰ˆæœ¬ï¼ˆæ¯æ¬¡å®Œæ•´æŠ“å– + è¿‡æ»¤ï¼‰"""

import json
import time
import os
from datetime import datetime, timedelta
import feedparser
from openai import OpenAI

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://ai.hybgzs.com/v1')
MODEL_NAME = os.getenv('MODEL_NAME', 'gemini-3-flash-preview')

if not OPENAI_API_KEY:
    raise ValueError('OPENAI_API_KEY environment variable is required')

BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))
BATCH_DELAY = int(os.getenv('BATCH_DELAY', '70'))
REQUEST_DELAY = float(os.getenv('REQUEST_DELAY', '0.2'))
CLASSIFY_BATCH_SIZE = int(os.getenv('CLASSIFY_BATCH_SIZE', '10'))
MAX_RETRIES = 3
RETRY_DELAY = 5

RSS_URLS = [
    "https://contraryresearch.substack.com/feed",
    "https://www.newsletter.datadrivenvc.io/feed",
    "https://seekingalpha.com/tag/editors-picks.xml",
    "https://icemancapital.substack.com/feed",
    "https://www.ft.com/rss/home",
    "https://www.levervc.com/feed/",
    "https://mbideepdives.substack.com/feed",
    "http://www.technologyreview.com/rss/rss.aspx",
    "https://www.newcomer.co/feed",
    "https://cdn.feedcontrol.net/8/1114-wioSIX3uu8MEj.xml",
    "https://svrgn.substack.com/feed",
    "https://www.speedwellmemos.com/feed",
    "https://techcrunch.com/feed/",
    "https://feeds.content.dowjones.io/public/rss/RSSWSJD",
    "https://attackcapital.substack.com/feed",
    "https://ritholtz.com/feed/rss",
    "https://thegeneralist.substack.com/feed",
    "https://www.thelastbearstanding.com/feed",
    "https://newsletter.tidalwaveresearch.com/feed",
    "http://blog.validea.com/feed/",
    "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
    "https://www.appinn.com/feed/",
    "https://sspai.com/feed",
    "https://bloombergnew.buzzing.cc/feed.xml",
]

feedparser.USER_AGENT = 'Mozilla/5.0 (compatible; RSS-Filter-Railway/1.0)'

def fetch_rss(url):
    try:
        feed = feedparser.parse(url)
        if not feed or not hasattr(feed, 'entries'):
            print(f"  âš ï¸  æ— æ•ˆå“åº”: {url}")
            return []
        if not feed.entries:
            print(f"  âš ï¸  æ²¡æœ‰entries: {url}")
            return []
        print(f"  âœ… è·å– {len(feed.entries)} æ¡æ–°é—»")
        return feed.entries
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ {url}: {e}")
        return []

def classify_batch(items, client):
    payload = []
    for idx, it in enumerate(items, 1):
        payload.append(
            {
                "id": idx,
                "title": it.get("title", ""),
                "summary": it.get("summary", "") or "æ— æ‘˜è¦",
            }
        )

    instructions = (
        "è¯·åˆ¤æ–­ä»¥ä¸‹æ¯æ¡æ–°é—»æ˜¯å¦ä¸è¿™äº›ä¸»é¢˜ç›¸å…³ï¼š"
        "Social networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰ã€‚\n"
        "è¯·ä¸¥æ ¼æŒ‰é¡ºåºè¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ•°ç»„é•¿åº¦å¿…é¡»ç­‰äºè¾“å…¥æ¡ç›®æ•°ã€‚\n"
        "æ•°ç»„å…ƒç´ åªèƒ½æ˜¯å­—ç¬¦ä¸² 'YES' æˆ– 'NO'ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€ä»£ç å—æˆ–å¤šä½™æ–‡å­—ã€‚"
    )

    prompt = f"{instructions}\n\nè¾“å…¥(JSON):\n{json.dumps(payload, ensure_ascii=False)}"

    last_error = None
    for retry in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è¿‡æ»¤å™¨ï¼Œåˆ¤æ–­æ–°é—»æ˜¯å¦ä¸ç‰¹å®šä¸»é¢˜ç›¸å…³ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=max(50, 8 * len(items)),
            )

            raw = response.choices[0].message.content.strip()
            arr = json.loads(raw)
            if not isinstance(arr, list) or len(arr) != len(items):
                return [None for _ in items]

            out = []
            for v in arr:
                if isinstance(v, str):
                    u = v.strip().upper()
                    if u == "YES":
                        out.append(True)
                        continue
                    if u == "NO":
                        out.append(False)
                        continue
                out.append(None)
            return out
        except Exception as e:
            last_error = str(e)
            if '429' in str(e) and retry < MAX_RETRIES - 1:
                sleep_s = RETRY_DELAY * (2 ** retry)
                print(
                    f"  â³ API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_s} ç§’åé‡è¯•... ({retry + 1}/{MAX_RETRIES})"
                )
                time.sleep(sleep_s)
                continue

            print(f"  âŒ LLM åˆ¤æ–­å¤±è´¥: {e}")
            return [None for _ in items]

    print(f"  âŒ LLM åˆ¤æ–­å¤±è´¥(å¤šæ¬¡é‡è¯•åä»å¤±è´¥): {last_error}")
    return [None for _ in items]

def main():
    print("ğŸš€ RSS è¿‡æ»¤å™¨ Railway ç‰ˆæœ¬å¯åŠ¨ä¸­...\n")

    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

    print(f"ğŸ“Š RSS æºæ•°é‡: {len(RSS_URLS)}")
    print(f"ğŸ”„ åˆ†æ‰¹å¤„ç†: æ¯æ‰¹ {BATCH_SIZE} æ¡ï¼Œæ‰¹æ¬¡é—´éš” {BATCH_DELAY} ç§’\n")

    all_entries = []

    for i, url in enumerate(RSS_URLS, 1):
        print(f"[{i}/{len(RSS_URLS)}] æŠ“å–: {url}")

        entries = fetch_rss(url)
        all_entries.extend(entries)

        time.sleep(0.5)

    print(f"\nğŸ“ æ€»å…±è·å– {len(all_entries)} æ¡æ–°é—»\n")

    seen_urls = set()
    unique_entries = []

    for entry in all_entries:
        url = entry.get('link', '')
        published = entry.get('published_parsed')

        if not url:
            continue

        if url in seen_urls:
            continue

        if published and datetime(*published[:6]) < datetime.now() - timedelta(days=3):
            continue

        seen_urls.add(url)
        unique_entries.append(entry)

    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(unique_entries)} æ¡\n")

    total_batches = (len(unique_entries) + BATCH_SIZE - 1) // BATCH_SIZE
    all_relevant = []
    all_unknown = []

    for batch_num in range(total_batches):
        start_idx = batch_num * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(unique_entries))
        batch_entries = unique_entries[start_idx:end_idx]

        print(f"{'='*60}")
        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches}")
        print(f"{'='*60}")
        print(f"èŒƒå›´: {start_idx + 1} - {end_idx} (å…± {len(batch_entries)} æ¡)\n")

        batch_relevant = []

        pending = []
        for i, entry in enumerate(batch_entries, start_idx + 1):
            title = entry.get('title', '')
            link = entry.get('link', '')
            published = entry.get('published', '')
            summary = entry.get('summary', '')[:200] if entry.get('summary') else ''

            pending.append(
                {
                    "i": i,
                    "title": title,
                    "link": link,
                    "published": published,
                    "summary": summary,
                }
            )

            if len(pending) < CLASSIFY_BATCH_SIZE and i != end_idx:
                continue

            verdicts = classify_batch(
                [{"title": p["title"], "summary": p["summary"]} for p in pending],
                client,
            )

            for p, verdict in zip(pending, verdicts):
                print(f"[{p['i']}/{len(unique_entries)}] {p['title'][:60]}...", end=" ")

                if verdict is True:
                    batch_relevant.append(
                        {
                            'title': p['title'],
                            'link': p['link'],
                            'published': p['published'],
                            'summary': p['summary']
                        }
                    )
                    print("âœ… ç›¸å…³")
                elif verdict is False:
                    print("â­ï¸  ä¸ç›¸å…³")
                else:
                    all_unknown.append(
                        {
                            'title': p['title'],
                            'link': p['link'],
                            'published': p['published'],
                            'summary': p['summary'],
                            'llm_status': 'unknown'
                        }
                    )
                    print("âš ï¸  æœªåˆ¤æ–­(é€Ÿç‡é™åˆ¶)")

            pending = []
            time.sleep(REQUEST_DELAY)

        all_relevant.extend(batch_relevant)

        print(f"\nâœ… æœ¬æ‰¹æ¬¡å®Œæˆ: æ‰¾åˆ° {len(batch_relevant)} æ¡ç›¸å…³æ–°é—»")
        denom = len(unique_entries) if len(unique_entries) else 1
        print(f"ğŸ“Š ç´¯è®¡ç›¸å…³: {len(all_relevant)}/{len(unique_entries)} æ¡ ({len(all_relevant)/denom*100:.1f}%)")
        if all_unknown:
            print(f"âš ï¸  æœªèƒ½åˆ¤æ–­(é€Ÿç‡é™åˆ¶ç­‰åŸå› ): {len(all_unknown)} æ¡")
        print("")

        if batch_num < total_batches - 1:
            print(f"â³ ç­‰å¾… {BATCH_DELAY} ç§’åç»§ç»­ä¸‹ä¸€æ‰¹...\n")
            time.sleep(BATCH_DELAY)

    print(f"{'='*60}")
    print(f"âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"{'='*60}\n")

    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - RSS æºæ€»æ•°: {len(RSS_URLS)}")
    print(f"   - æ€»æ–°é—»æ•°: {len(all_entries)}")
    print(f"   - å»é‡å: {len(unique_entries)}")
    print(f"   - ç›¸å…³æ–°é—»: {len(all_relevant)}")
    if all_unknown:
        print(f"   - æœªèƒ½åˆ¤æ–­: {len(all_unknown)}")
    print(f"   - ç›¸å…³æ¯”ä¾‹: {len(all_relevant)/len(unique_entries)*100:.2f}%")

    all_relevant.sort(key=lambda x: x['published'], reverse=True)

    md_content = f'''# ç›¸å…³æ–°é—»è¿‡æ»¤ç»“æœ

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
**è¿è¡Œå¹³å°**: Railway
**è¿‡æ»¤ä¸»é¢˜**: Social networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€Live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€Mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰
**ç›¸å…³æ–°é—»æ•°é‡**: {len(all_relevant)} æ¡
**æœªèƒ½åˆ¤æ–­æ•°é‡**: {len(all_unknown)} æ¡
**æ€»æ–°é—»æ•°**: {len(unique_entries)} æ¡
**è¿‡æ»¤æ¯”ä¾‹**: {len(all_relevant)/len(unique_entries)*100:.2f}%

---

## ğŸ“° æ–°é—»åˆ—è¡¨

'''

    for i, news in enumerate(all_relevant, 1):
        md_content += f'''
### {i}. {news['title']}

**é“¾æ¥**: [{news['link']}]({news['link']})
**å‘å¸ƒæ—¶é—´**: {news['published']}
**æ‘˜è¦**: {news.get('summary', 'æ— ')}

---

'''

    md_content += f'''

---

## ğŸ“Š å¤„ç†ç»Ÿè®¡

| ç»Ÿè®¡é¡¹ | æ•°å€¼ |
|---------|------|
| RSS æºæ€»æ•° | {len(RSS_URLS)} ä¸ª |
| æ€»æ–°é—»æ•° | {len(all_entries)} æ¡ |
| å»é‡åæ–°é—» | {len(unique_entries)} æ¡ |
| ç›¸å…³æ–°é—» | {len(all_relevant)} æ¡ |
| æœªèƒ½åˆ¤æ–­ | {len(all_unknown)} æ¡ |
| ç›¸å…³æ¯”ä¾‹ | {len(all_relevant)/len(unique_entries)*100:.2f}% |
| LLM æ¨¡å‹ | {MODEL_NAME} |
| API æä¾›å•† | {OPENAI_BASE_URL} |

---

*æœ¬æŠ¥å‘Šç”± AI RSS è¿‡æ»¤å™¨è‡ªåŠ¨ç”Ÿæˆ*
*è¿è¡Œå¹³å°: Railway*
*RSS æº: 24 ä¸ªç²¾é€‰æº*
*å¤„ç†æ‰¹æ¬¡: {total_batches} æ‰¹*
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
'''

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_file = f"filtered_news_{timestamp}.json"
    unknown_json_file = f"unknown_news_{timestamp}.json"
    md_file = f"ç›¸å…³æ–°é—»_{timestamp}.md"

    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_relevant, f, ensure_ascii=False, indent=2)

    if all_unknown:
        with open(unknown_json_file, 'w', encoding='utf-8') as f:
            json.dump(all_unknown, f, ensure_ascii=False, indent=2)

    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"\nâœ… ç»“æœå·²ä¿å­˜:")
    print(f"   ğŸ“„ JSON: {json_file}")
    if all_unknown:
        print(f"   ğŸ“„ Unknown JSON: {unknown_json_file}")
    print(f"   ğŸ“„ Markdown: {md_file}")

if __name__ == "__main__":
    main()
