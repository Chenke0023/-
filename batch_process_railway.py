#!/usr/bin/env python3
"""RSS æ–°é—»è¿‡æ»¤å™¨ - Railway ç‰ˆæœ¬ï¼ˆæ¯æ¬¡å®Œæ•´æŠ“å– + è¿‡æ»¤ï¼‰"""

import json
import time
import os
from datetime import datetime, timedelta
import feedparser
from openai import OpenAI

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'sk-EjDBhZm5xTqkXfe_ea_iIUpuls7IUT5ZmTTufteiR5qlyHwCO6l0k3Kh1oE')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://ai.hybgzs.com/v1')
MODEL_NAME = os.getenv('MODEL_NAME', 'gemini-3-flash-preview')

BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))
BATCH_DELAY = int(os.getenv('BATCH_DELAY', '10'))
REQUEST_DELAY = float(os.getenv('REQUEST_DELAY', '0.2'))
MAX_RETRIES = 3
RETRY_DELAY = 5

RSS_URLS = [
    "https://contraryresearch.substack.com/feed",
    "https://www.newsletter.datadrivenvc.io/feed",
    "https://seekingalpha.com/tag/editors-picks.xml",
    "https://icemancapital.substack.com/feed",
    "https://www.ft.com/rss/home",
    "https://www.levervc.com/feed/",
    "https://mbideepdives.substack.com/feed",
    "http://www.technologyreview.com/rss/rss.aspx",
    "https://www.newcomer.co/feed",
    "https://cdn.feedcontrol.net/8/1114-wioSIX3uu8MEj.xml",
    "https://svrgn.substack.com/feed",
    "https://www.speedwellmemos.com/feed",
    "https://techcrunch.com/feed/",
    "https://feeds.content.dowjones.io/public/rss/RSSWSJD",
    "https://attackcapital.substack.com/feed",
    "https://ritholtz.com/feed/rss",
    "https://thegeneralist.substack.com/feed",
    "https://www.thelastbearstanding.com/feed",
    "https://newsletter.tidalwaveresearch.com/feed",
    "http://blog.validea.com/feed/",
    "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
    "https://www.appinn.com/feed/",
    "https://sspai.com/feed",
    "https://bloombergnew.buzzing.cc/feed.xml",
]

feedparser.USER_AGENT = 'Mozilla/5.0 (compatible; RSS-Filter-Railway/1.0)'

def fetch_rss(url):
    try:
        feed = feedparser.parse(url)
        if not feed or not hasattr(feed, 'entries'):
            print(f"  âš ï¸  æ— æ•ˆå“åº”: {url}")
            return []
        if not feed.entries:
            print(f"  âš ï¸  æ²¡æœ‰entries: {url}")
            return []
        print(f"  âœ… è·å– {len(feed.entries)} æ¡æ–°é—»")
        return feed.entries
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ {url}: {e}")
        return []

def is_relevant_with_llm(title, summary, client):
    prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹æ–°é—»æ˜¯å¦ä¸è¿™äº›ä¸»é¢˜ç›¸å…³ï¼šSocial networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰ã€‚

æ–°é—»æ ‡é¢˜: {title}
æ–°é—»æ‘˜è¦: {summary if summary else 'æ— æ‘˜è¦'}

è¯·åªå›ç­” YES æˆ– NOï¼Œä¸éœ€è¦è§£é‡Šã€‚"""

    for retry in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è¿‡æ»¤å™¨ï¼Œåˆ¤æ–­æ–°é—»æ˜¯å¦ä¸ç‰¹å®šä¸»é¢˜ç›¸å…³ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=10
            )

            answer = response.choices[0].message.content.strip().upper()
            return answer == "YES"
        except Exception as e:
            if '429' in str(e) and retry < MAX_RETRIES - 1:
                print(f"  â³ API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {RETRY_DELAY} ç§’åé‡è¯•... ({retry + 1}/{MAX_RETRIES})")
                time.sleep(RETRY_DELAY)
                continue
            else:
                print(f"  âŒ LLM åˆ¤æ–­å¤±è´¥: {e}")
                return False
    return False

def main():
    print("ğŸš€ RSS è¿‡æ»¤å™¨ Railway ç‰ˆæœ¬å¯åŠ¨ä¸­...\n")

    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

    print(f"ğŸ“Š RSS æºæ•°é‡: {len(RSS_URLS)}")
    print(f"ğŸ”„ åˆ†æ‰¹å¤„ç†: æ¯æ‰¹ {BATCH_SIZE} æ¡ï¼Œæ‰¹æ¬¡é—´éš” {BATCH_DELAY} ç§’\n")

    all_entries = []

    for i, url in enumerate(RSS_URLS, 1):
        print(f"[{i}/{len(RSS_URLS)}] æŠ“å–: {url}")

        entries = fetch_rss(url)
        all_entries.extend(entries)

        time.sleep(0.5)

    print(f"\nğŸ“ æ€»å…±è·å– {len(all_entries)} æ¡æ–°é—»\n")

    seen_urls = set()
    unique_entries = []

    for entry in all_entries:
        url = entry.get('link', '')
        published = entry.get('published_parsed')

        if not url:
            continue

        if url in seen_urls:
            continue

        if published and datetime(*published[:6]) < datetime.now() - timedelta(days=3):
            continue

        seen_urls.add(url)
        unique_entries.append(entry)

    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(unique_entries)} æ¡\n")

    total_batches = (len(unique_entries) + BATCH_SIZE - 1) // BATCH_SIZE
    all_relevant = []

    for batch_num in range(total_batches):
        start_idx = batch_num * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(unique_entries))
        batch_entries = unique_entries[start_idx:end_idx]

        print(f"{'='*60}")
        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches}")
        print(f"{'='*60}")
        print(f"èŒƒå›´: {start_idx + 1} - {end_idx} (å…± {len(batch_entries)} æ¡)\n")

        batch_relevant = []

        for i, entry in enumerate(batch_entries, start_idx + 1):
            title = entry.get('title', '')
            link = entry.get('link', '')
            published = entry.get('published', '')
            summary = entry.get('summary', '')[:200] if entry.get('summary') else ''

            print(f"[{i}/{len(unique_entries)}] {title[:60]}...", end=" ")

            if is_relevant_with_llm(title, summary, client):
                batch_relevant.append({
                    'title': title,
                    'link': link,
                    'published': published,
                    'summary': summary
                })
                print("âœ… ç›¸å…³")
            else:
                print("â­ï¸  ä¸ç›¸å…³")

            time.sleep(REQUEST_DELAY)

        all_relevant.extend(batch_relevant)

        print(f"\nâœ… æœ¬æ‰¹æ¬¡å®Œæˆ: æ‰¾åˆ° {len(batch_relevant)} æ¡ç›¸å…³æ–°é—»")
        print(f"ğŸ“Š ç´¯è®¡ç›¸å…³: {len(all_relevant)}/{len(unique_entries)} æ¡ ({len(all_relevant)/len(unique_entries)*100:.1f}%)\n")

        if batch_num < total_batches - 1:
            print(f"â³ ç­‰å¾… {BATCH_DELAY} ç§’åç»§ç»­ä¸‹ä¸€æ‰¹...\n")
            time.sleep(BATCH_DELAY)

    print(f"{'='*60}")
    print(f"âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"{'='*60}\n")

    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - RSS æºæ€»æ•°: {len(RSS_URLS)}")
    print(f"   - æ€»æ–°é—»æ•°: {len(all_entries)}")
    print(f"   - å»é‡å: {len(unique_entries)}")
    print(f"   - ç›¸å…³æ–°é—»: {len(all_relevant)}")
    print(f"   - ç›¸å…³æ¯”ä¾‹: {len(all_relevant)/len(unique_entries)*100:.2f}%")

    all_relevant.sort(key=lambda x: x['published'], reverse=True)

    md_content = f'''# ç›¸å…³æ–°é—»è¿‡æ»¤ç»“æœ

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
**è¿è¡Œå¹³å°**: Railway
**è¿‡æ»¤ä¸»é¢˜**: Social networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€Live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€Mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰
**ç›¸å…³æ–°é—»æ•°é‡**: {len(all_relevant)} æ¡
**æ€»æ–°é—»æ•°**: {len(unique_entries)} æ¡
**è¿‡æ»¤æ¯”ä¾‹**: {len(all_relevant)/len(unique_entries)*100:.2f}%

---

## ğŸ“° æ–°é—»åˆ—è¡¨

'''

    for i, news in enumerate(all_relevant, 1):
        md_content += f'''
### {i}. {news['title']}

**é“¾æ¥**: [{news['link']}]({news['link']})
**å‘å¸ƒæ—¶é—´**: {news['published']}
**æ‘˜è¦**: {news.get('summary', 'æ— ')}

---

'''

    md_content += f'''

---

## ğŸ“Š å¤„ç†ç»Ÿè®¡

| ç»Ÿè®¡é¡¹ | æ•°å€¼ |
|---------|------|
| RSS æºæ€»æ•° | {len(RSS_URLS)} ä¸ª |
| æ€»æ–°é—»æ•° | {len(all_entries)} æ¡ |
| å»é‡åæ–°é—» | {len(unique_entries)} æ¡ |
| ç›¸å…³æ–°é—» | {len(all_relevant)} æ¡ |
| ç›¸å…³æ¯”ä¾‹ | {len(all_relevant)/len(unique_entries)*100:.2f}% |
| LLM æ¨¡å‹ | {MODEL_NAME} |
| API æä¾›å•† | {OPENAI_BASE_URL} |

---

*æœ¬æŠ¥å‘Šç”± AI RSS è¿‡æ»¤å™¨è‡ªåŠ¨ç”Ÿæˆ*
*è¿è¡Œå¹³å°: Railway*
*RSS æº: 24 ä¸ªç²¾é€‰æº*
*å¤„ç†æ‰¹æ¬¡: {total_batches} æ‰¹*
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
'''

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_file = f"filtered_news_{timestamp}.json"
    md_file = f"ç›¸å…³æ–°é—»_{timestamp}.md"

    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_relevant, f, ensure_ascii=False, indent=2)

    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"\nâœ… ç»“æœå·²ä¿å­˜:")
    print(f"   ğŸ“„ JSON: {json_file}")
    print(f"   ğŸ“„ Markdown: {md_file}")

if __name__ == "__main__":
    main()
