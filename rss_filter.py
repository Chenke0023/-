#!/usr/bin/env python3

import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict
from urllib.parse import urlparse
import feedparser
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

feedparser.USER_AGENT = 'Mozilla/5.0 (compatible; RSS-Filter/1.0)'

API_URL = 'https://ai.hybgzs.com/v1'
API_KEY = 'sk-EjDBhZm5xTqkXfe_ea_iIUpuls7IUT5ZmTTufteiR5qlyHwCO6l0k3Kh1oE'
MODEL = 'gemini-3-flash-preview'

def get_rss_urls():
    try:
        with open("extracted_urls.py", "r") as f:
            content = f.read()
            urls = content.split('RSS_URLS = [')[1].split(']')[0]
            return [url.strip().strip('"').strip("'") for url in urls.split(',')]
    except:
        return [
            "https://contraryresearch.substack.com/feed",
            "https://www.newsletter.datadrivenvc.io/feed",
            "https://seekingalpha.com/tag/editors-picks.xml",
            "https://icemancapital.substack.com/feed",
            "https://www.ft.com/rss/home",
            "https://www.levervc.com/feed/",
            "https://mbideepdives.substack.com/feed",
            "http://www.technologyreview.com/rss/rss.aspx",
            "https://www.newcomer.co/feed",
            "https://cdn.feedcontrol.net/8/1114-wioSIX3uu8MEj.xml",
            "https://svrgn.substack.com/feed",
            "https://www.speedwellmemos.com/feed",
            "https://techcrunch.com/feed/",
            "https://feeds.content.dowjones.io/public/rss/RSSWSJD",
            "https://attackcapital.substack.com/feed",
            "https://ritholtz.com/feed/rss",
            "https://thegeneralist.substack.com/feed",
            "https://www.thelastbearstanding.com/feed",
            "https://newsletter.tidalwaveresearch.com/feed",
            "http://blog.validea.com/feed/",
            "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
            "https://www.appinn.com/feed/",
            "https://sspai.com/feed",
            "https://bloombergnew.buzzing.cc/feed.xml",
        ]

def fetch_rss(url):
    try:
        feed = feedparser.parse(url)
        if not feed or not hasattr(feed, 'entries'):
            print(f"  âš ï¸  æ— æ•ˆå“åº”: {url}")
            return None
        if not feed.entries:
            print(f"  âš ï¸  æ²¡æœ‰entries: {url}")
            return None
        print(f"  âœ… è·å– {len(feed.entries)} æ¡æ–°é—»")
        return feed
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥ {url}: {e}")
        import traceback
        traceback.print_exc()
        return None

def is_relevant_with_llm(title, summary, client):
    prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹æ–°é—»æ˜¯å¦ä¸è¿™äº›ä¸»é¢˜ç›¸å…³ï¼šSocial networkingï¼ˆç¤¾äº¤ç½‘ç»œï¼‰ã€live streamingï¼ˆç›´æ’­ï¼‰ã€TMT acquisitionsï¼ˆTMTå¹¶è´­ï¼‰ã€mobile gamingï¼ˆæ‰‹æœºæ¸¸æˆï¼‰ã€‚

æ–°é—»æ ‡é¢˜: {title}
æ–°é—»æ‘˜è¦: {summary if summary else 'æ— æ‘˜è¦'}

è¯·åªå›ç­” YES æˆ– NOï¼Œä¸éœ€è¦è§£é‡Šã€‚"""

    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è¿‡æ»¤å™¨ï¼Œåˆ¤æ–­æ–°é—»æ˜¯å¦ä¸ç‰¹å®šä¸»é¢˜ç›¸å…³ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=10
        )

        answer = response.choices[0].message.content.strip().upper()
        return answer == "YES"
    except Exception as e:
        print(f"  âŒ LLM åˆ¤æ–­å¤±è´¥: {e}")
        return False

def deduplicate(entries, seen_urls, days=3):
    cutoff_date = datetime.now() - timedelta(days=days)
    filtered = []

    for entry in entries:
        url = entry.get('link', '')
        published = entry.get('published_parsed')

        if not url:
            continue

        if url in seen_urls:
            continue

        if published and datetime(*published[:6]) < cutoff_date:
            continue

        seen_urls.add(url)
        filtered.append(entry)

    return filtered

def main():
    print("ğŸš€ RSS æ–°é—»è¿‡æ»¤å™¨å¯åŠ¨ä¸­...\n")

    client = OpenAI(
        api_key=API_KEY,
        base_url=API_URL
    )

    rss_urls = get_rss_urls()
    print(f"ğŸ“Š åŠ è½½ {len(rss_urls)} ä¸ª RSS æº\n")

    all_entries = []

    for i, url in enumerate(rss_urls, 1):
        print(f"[{i}/{len(rss_urls)}] æŠ“å–: {urlparse(url).netloc}")
        feed = fetch_rss(url)

        if feed and feed.get('entries'):
            entries = feed.entries[:20]
            print(f"  âœ… è·å– {len(entries)} æ¡æ–°é—»")
            all_entries.extend(entries)
        else:
            print(f"  âš ï¸  æ²¡æœ‰å†…å®¹æˆ–å¤±è´¥")

    print(f"\nğŸ“ æ€»å…±è·å– {len(all_entries)} æ¡æ–°é—»")

    seen_urls = set()
    deduplicated = deduplicate(all_entries, seen_urls)
    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(deduplicated)} æ¡\n")

    relevant_news = []

    for i, entry in enumerate(deduplicated, 1):
        title = entry.get('title', 'æ— æ ‡é¢˜')
        link = entry.get('link', '')
        summary = entry.get('summary', '')[:200]

        print(f"[{i}/{len(deduplicated)}] åˆ¤æ–­: {title[:50]}...")

        if is_relevant_with_llm(title, summary, client):
            relevant_news.append({
                'title': title,
                'link': link,
                'published': entry.get('published', ''),
                'source': entry.get('feed', {}).get('title', 'Unknown')
            })
            print(f"  âœ… ç›¸å…³ - {title[:30]}...")
        else:
            print(f"  â­ï¸  ä¸ç›¸å…³")

        time.sleep(0.5)

    print(f"\nğŸ¯ æ‰¾åˆ° {len(relevant_news)} æ¡ç›¸å…³æ–°é—»")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"filtered_news_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(relevant_news, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_file}")

    if relevant_news:
        print(f"\nğŸ“° ç›¸å…³æ–°é—»é¢„è§ˆ:")
        for news in relevant_news[:5]:
            print(f"\n  ğŸ“Œ {news['title']}")
            print(f"     ğŸ”— {news['link']}")
            print(f"     ğŸ“… {news['published']}")
            print(f"     ğŸ“° æ¥æº: {news['source']}")

if __name__ == "__main__":
    main()
